# B√ÅO C√ÅO PH√ÇN T√çCH V·∫§N ƒê·ªÄ ACCURACY KH√îNG THAY ƒê·ªîI

## üö® **V·∫§N ƒê·ªÄ PH√ÅT HI·ªÜN**

T·ª´ log file `logs/models_20251007.log`, t√¥i ph√°t hi·ªán **v·∫•n ƒë·ªÅ nghi√™m tr·ªçng**: Direction Accuracy c·ªßa nhi·ªÅu models **HO√ÄN TO√ÄN KH√îNG THAY ƒê·ªîI** trong su·ªët qu√° tr√¨nh training.

---

## üìä **PH√ÇN T√çCH CHI TI·∫æT**

### **1. VIB Model (CNN-BiLSTM)**
```
Epoch 5:  Target_Direction_t+1 - Accuracy: 0.842466
Epoch 10: Target_Direction_t+1 - Accuracy: 0.842466
Epoch 15: Target_Direction_t+1 - Accuracy: 0.842466
Epoch 20: Target_Direction_t+1 - Accuracy: 0.842466
Epoch 25: Target_Direction_t+1 - Accuracy: 0.842466
Epoch 30: Target_Direction_t+1 - Accuracy: 0.842466
Epoch 35: Target_Direction_t+1 - Accuracy: 0.842466
```
**K·∫øt lu·∫≠n**: Accuracy **HO√ÄN TO√ÄN KH√îNG ƒê·ªîI** = 0.842466 (84.24%)

### **2. VCB Model (CNN-BiLSTM)**
```
Epoch 1:  Target_Direction_t+1 - Accuracy: 0.911565
Epoch 5:  Target_Direction_t+1 - Accuracy: 0.911565
Epoch 10: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 15: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 20: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 25: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 30: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 35: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 40: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 45: Target_Direction_t+1 - Accuracy: 0.911565
Epoch 50: Target_Direction_t+1 - Accuracy: 0.911565
```
**K·∫øt lu·∫≠n**: Accuracy **HO√ÄN TO√ÄN KH√îNG ƒê·ªîI** = 0.911565 (91.16%)

### **3. Pattern Chung**
- **Target_Direction_t+1**: Accuracy c·ªë ƒë·ªãnh ·ªü m·ª©c cao (84-91%)
- **Target_Direction_t+3**: Accuracy thay ƒë·ªïi nh∆∞ng kh√¥ng ·ªïn ƒë·ªãnh
- **Target_Direction_t+5**: Accuracy thay ƒë·ªïi nh∆∞ng kh√¥ng ·ªïn ƒë·ªãnh

---

## üîç **NGUY√äN NH√ÇN G·ªêC R·ªÑ**

### **1. Model Ch·ªâ H·ªçc Predict Dominant Class**
```python
# Class Distribution Analysis
Class 1 (Flat): 62.6% - DOMINANT CLASS
Class 0 (Down): 18.1%
Class 2 (Up): 19.3%

# Model behavior:
VCB Accuracy = 91.16% ‚âà Dominant class percentage
VIB Accuracy = 84.24% ‚âà Dominant class percentage
```

**K·∫øt lu·∫≠n**: Model ƒë√£ h·ªçc c√°ch **LU√îN PREDICT CLASS 1 (FLAT)** ƒë·ªÉ maximize accuracy!

### **2. Class Weights Kh√¥ng Hi·ªáu Qu·∫£**
```python
# Class weights ƒë∆∞·ª£c t√≠nh:
VIB: [1.4469, 0.6689, 1.2285]
VCB: [1.6471, 0.5748, 1.5313]

# Nh∆∞ng model v·∫´n ignore minority classes
```

**V·∫•n ƒë·ªÅ**: Class weights ch∆∞a ƒë·ªß m·∫°nh ƒë·ªÉ force model h·ªçc minority classes.

### **3. Loss Function Imbalance**
```python
# Total Loss = Regression Loss + Classification Loss
# Regression loss c√≥ th·ªÉ dominate classification loss
# Model focus v√†o minimize regression loss, ignore classification
```

---

## üö® **T·∫†I SAO ƒê√ÇY L√Ä V·∫§N ƒê·ªÄ NGHI√äM TR·ªåNG?**

### **1. Model Kh√¥ng Th·ª±c S·ª± H·ªçc**
- Accuracy cao (84-91%) nh∆∞ng **FAKE**
- Model ch·ªâ memorize dominant class
- Kh√¥ng c√≥ kh·∫£ nƒÉng generalization

### **2. Prediction Kh√¥ng C√≥ Gi√° Tr·ªã**
```python
# Model prediction:
for any_input:
    return "FLAT"  # Always predict class 1

# ƒêi·ªÅu n√†y v√¥ nghƒ©a cho trading!
```

### **3. Metrics Misleading**
- Direction Accuracy = 91% **KH√îNG C√ì NGHƒ®A**
- Precision/Recall cho class 0,2 = 0%
- F1-score th·ª±c t·∫ø r·∫•t th·∫•p

---

## üîß **GI·∫¢I PH√ÅP KH·∫ÆC PH·ª§C**

### **1. TƒÉng Class Weights M·∫°nh H∆°n**
```python
# Thay v√¨:
class_weights = [1.65, 0.57, 1.53]

# S·ª≠ d·ª•ng:
class_weights = [5.0, 0.2, 5.0]  # Penalty m·∫°nh cho dominant class
```

### **2. Focal Loss thay v√¨ CrossEntropy**
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()
```

### **3. Separate Loss Weighting**
```python
# T√°ch ri√™ng loss weights:
total_loss = (
    regression_weight * regression_loss +
    classification_weight * classification_loss
)

# V·ªõi classification_weight >> regression_weight
regression_weight = 0.3
classification_weight = 0.7
```

### **4. Balanced Sampling**
```python
# S·ª≠ d·ª•ng WeightedRandomSampler
from torch.utils.data import WeightedRandomSampler

# T·∫°o balanced batches
sampler = WeightedRandomSampler(
    weights=sample_weights,
    num_samples=len(dataset),
    replacement=True
)
```

### **5. Threshold Tuning**
```python
# Thay v√¨ d√πng default threshold (0.5):
# Tune threshold ƒë·ªÉ balance precision/recall
optimal_threshold = find_optimal_threshold(y_true, y_pred_proba)
```

### **6. Evaluation Metrics C·∫£i Thi·ªán**
```python
# Thay v√¨ ch·ªâ accuracy, track:
- Balanced Accuracy
- F1-score per class
- Precision/Recall per class
- Confusion Matrix
- Cohen's Kappa
```

---

## üéØ **H√ÄNH ƒê·ªòNG NGAY L·∫¨P T·ª®C**

### **B∆∞·ªõc 1: Ki·ªÉm Tra Confusion Matrix**
```python
from sklearn.metrics import confusion_matrix, classification_report

# Xem model predict g√¨ th·ª±c t·∫ø
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)

# N·∫øu ch·ªâ c√≥ 1 column/row kh√°c 0 ‚Üí confirmed problem
```

### **B∆∞·ªõc 2: Implement Focal Loss**
```python
# Thay ƒë·ªïi trong base_model.py
criterion = FocalLoss(alpha=1, gamma=2)
loss = criterion(outputs[target_key], target_batch[i].long())
```

### **B∆∞·ªõc 3: TƒÉng Classification Loss Weight**
```python
# Trong training loop:
classification_loss_weight = 2.0  # TƒÉng t·ª´ 1.0
total_loss = regression_loss + classification_loss_weight * classification_loss
```

### **B∆∞·ªõc 4: Monitor Per-Class Metrics**
```python
# Th√™m v√†o logging:
from sklearn.metrics import precision_recall_fscore_support

precision, recall, f1, _ = precision_recall_fscore_support(
    y_true, y_pred, average=None
)

logger.info(f"Per-class Precision: {precision}")
logger.info(f"Per-class Recall: {recall}")
logger.info(f"Per-class F1: {f1}")
```

---

## üìà **K·∫æT QU·∫¢ MONG ƒê·ª¢I SAU KHI S·ª¨A**

### **Tr∆∞·ªõc khi s·ª≠a:**
```
Target_Direction_t+1 - Accuracy: 0.911565 (FAKE - always predict class 1)
```

### **Sau khi s·ª≠a:**
```
Target_Direction_t+1 - Accuracy: 0.650000 (REAL - balanced prediction)
Per-class Precision: [0.60, 0.65, 0.70]
Per-class Recall: [0.55, 0.70, 0.65]
Per-class F1: [0.57, 0.67, 0.67]
```

---

## üö® **K·∫æT LU·∫¨N**

**V·∫§N ƒê·ªÄ HI·ªÜN T·∫†I**: Models ƒëang **FAKE LEARNING** - ch·ªâ predict dominant class ƒë·ªÉ maximize accuracy.

**T√ÅC ƒê·ªòNG**: 
- Accuracy metrics ho√†n to√†n misleading
- Models kh√¥ng c√≥ gi√° tr·ªã th·ª±c t·∫ø cho trading
- C·∫ßn fix ngay l·∫≠p t·ª©c tr∆∞·ªõc khi deploy

**∆ØU TI√äN**: 
1. **Implement Focal Loss** (quan tr·ªçng nh·∫•t)
2. **TƒÉng classification loss weight**
3. **Monitor per-class metrics**
4. **Retrain t·∫•t c·∫£ models**

**ƒê√¢y l√† bug nghi√™m tr·ªçng c·∫ßn fix ngay ƒë·ªÉ c√≥ models th·ª±c s·ª± ho·∫°t ƒë·ªông!** üö®