#!/usr/bin/env python3
"""
Check Pipeline Results and Model Performance
Comprehensive results checker for banking stock prediction models
"""
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import glob
import yaml

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

try:
    from src.utils.config import get_config
    from src.utils.database import get_database
except ImportError:
    print("âš ï¸ Could not import project modules. Some features may be limited.")

def print_header(title):
    """Print formatted header"""
    print(f"\n{'='*60}")
    print(f"{title}")
    print(f"{'='*60}")

def print_section(title):
    """Print formatted section"""
    print(f"\n{'-'*40}")
    print(f"{title}")
    print(f"{'-'*40}")

def check_data_status():
    """Check data collection status"""
    print_section("Data Collection Status")
    
    # Check raw data
    raw_dir = "data/raw"
    if os.path.exists(raw_dir):
        raw_files = [f for f in os.listdir(raw_dir) if f.endswith('.csv')]
        print(f"ðŸ“ Raw data files: {len(raw_files)}")
        
        if raw_files:
            # Check latest file dates
            latest_dates = []
            for file in raw_files[:3]:  # Check first 3 files
                file_path = os.path.join(raw_dir, file)
                try:
                    df = pd.read_csv(file_path)
                    if 'time' in df.columns:
                        latest_date = pd.to_datetime(df['time']).max()
                        latest_dates.append(latest_date)
                        ticker = file.split('_')[0]
                        days_old = (pd.Timestamp.now() - latest_date).days
                        print(f"  ðŸ“ˆ {ticker}: {latest_date.strftime('%Y-%m-%d')} ({days_old} days old)")
                except Exception as e:
                    print(f"  âŒ Error reading {file}: {e}")
            
            if latest_dates:
                avg_age = np.mean([(pd.Timestamp.now() - date).days for date in latest_dates])
                print(f"ðŸ“… Average data age: {avg_age:.1f} days")
        else:
            print("âŒ No raw data files found")
    else:
        print("âŒ Raw data directory not found")
    
    # Check processed data
    processed_dir = "data/processed"
    if os.path.exists(processed_dir):
        processed_files = [f for f in os.listdir(processed_dir) if f.endswith('.csv')]
        sequences_files = [f for f in os.listdir(processed_dir) if f.endswith('.npz')]
        print(f"ðŸ“ Processed feature files: {len(processed_files)}")
        print(f"ðŸ“ Sequence files: {len(sequences_files)}")
    else:
        print("âŒ Processed data directory not found")

def check_model_status():
    """Check trained models status"""
    print_section("Model Training Status")
    
    models_dir = "models"
    if not os.path.exists(models_dir):
        print("âŒ Models directory not found")
        return {}
    
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pt')]
    print(f"ðŸ¤– Total model files: {len(model_files)}")
    
    if not model_files:
        print("âŒ No trained models found")
        return {}
    
    # Analyze models by type and ticker
    model_stats = {}
    model_types = ['lstm', 'cnn_bilstm', 'transformer']
    
    # Get tickers from config
    try:
        config = get_config()
        tickers = config.tickers
    except:
        # Fallback: extract from model files
        tickers = list(set([f.split('_')[0] for f in model_files]))
    
    print(f"ðŸ¦ Banks: {len(tickers)} ({', '.join(tickers[:5])}{'...' if len(tickers) > 5 else ''})")
    
    for model_type in model_types:
        type_models = [f for f in model_files if model_type in f]
        model_stats[model_type] = len(type_models)
        print(f"  ðŸ¤– {model_type.upper()}: {len(type_models)}/{len(tickers)} banks")
    
    # Check model file sizes and dates
    print(f"\nðŸ“Š Model File Analysis:")
    recent_models = sorted(model_files, key=lambda x: os.path.getmtime(os.path.join(models_dir, x)), reverse=True)[:5]
    
    for model_file in recent_models:
        file_path = os.path.join(models_dir, model_file)
        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
        mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
        hours_old = (datetime.now() - mod_time).total_seconds() / 3600
        
        print(f"  ðŸ“ {model_file}: {file_size:.1f}MB, {hours_old:.1f}h old")
    
    return model_stats

def analyze_training_logs():
    """Analyze training logs for performance metrics"""
    print_section("Training Performance Analysis")
    
    logs_dir = "logs"
    if not os.path.exists(logs_dir):
        print("âŒ Logs directory not found")
        return
    
    # Find latest training logs
    log_files = [f for f in os.listdir(logs_dir) if f.startswith('main_') and f.endswith('.log')]
    if not log_files:
        print("âŒ No training logs found")
        return
    
    latest_log = sorted(log_files)[-1]
    log_path = os.path.join(logs_dir, latest_log)
    
    print(f"ðŸ“„ Analyzing: {latest_log}")
    
    try:
        with open(log_path, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # Extract performance metrics
        metrics = extract_metrics_from_log(log_content)
        
        if metrics:
            display_performance_metrics(metrics)
        else:
            print("âš ï¸ No performance metrics found in logs")
            
        # Check for errors
        error_lines = [line for line in log_content.split('\n') if 'ERROR' in line or 'Failed' in line]
        if error_lines:
            print(f"\nâš ï¸ Found {len(error_lines)} error messages:")
            for error in error_lines[-3:]:  # Show last 3 errors
                print(f"  âŒ {error.strip()}")
        else:
            print("âœ… No errors found in training logs")
            
    except Exception as e:
        print(f"âŒ Error reading log file: {e}")

def extract_metrics_from_log(log_content):
    """Extract performance metrics from log content"""
    metrics = {}
    lines = log_content.split('\n')
    
    current_model = None
    
    for line in lines:
        line = line.strip()
        
        # Detect model type
        if any(model in line for model in ['LSTM:', 'CNN_BILSTM:', 'TRANSFORMER:']):
            current_model = line.split(':')[0]
            if current_model not in metrics:
                metrics[current_model] = {}
        
        # Extract metrics (RMSE, RÂ², Direction Accuracy)
        if current_model and ('RMSE=' in line and 'RÂ²=' in line):
            try:
                parts = line.split()
                ticker = parts[0]
                
                rmse, r2, direction_acc = None, None, None
                
                for part in parts:
                    if 'RMSE=' in part:
                        rmse = float(part.split('=')[1].replace(',', ''))
                    elif 'RÂ²=' in part:
                        r2 = float(part.split('=')[1])
                    elif 'Direction=' in part:
                        direction_acc = float(part.split('=')[1].replace('%', ''))
                
                if ticker not in metrics[current_model]:
                    metrics[current_model][ticker] = []
                
                metrics[current_model][ticker].append({
                    'rmse': rmse,
                    'r2': r2,
                    'direction_accuracy': direction_acc
                })
                
            except Exception as e:
                continue  # Skip malformed lines
    
    return metrics

def display_performance_metrics(metrics):
    """Display performance metrics in a formatted way"""
    print(f"\nðŸ“ˆ Performance Summary:")
    
    for model_type, model_data in metrics.items():
        print(f"\nðŸ¤– {model_type}:")
        
        all_r2 = []
        all_rmse = []
        all_direction = []
        
        for ticker, ticker_metrics in model_data.items():
            for metric in ticker_metrics:
                if metric['r2'] is not None:
                    all_r2.append(metric['r2'])
                if metric['rmse'] is not None:
                    all_rmse.append(metric['rmse'])
                if metric['direction_accuracy'] is not None:
                    all_direction.append(metric['direction_accuracy'])
        
        if all_r2:
            print(f"  ðŸ“Š RÂ² Score: {np.mean(all_r2):.4f} Â± {np.std(all_r2):.4f}")
            print(f"     Range: [{np.min(all_r2):.4f}, {np.max(all_r2):.4f}]")
            positive_r2 = sum(1 for r2 in all_r2 if r2 > 0)
            print(f"     Positive RÂ²: {positive_r2}/{len(all_r2)} ({positive_r2/len(all_r2)*100:.1f}%)")
        
        if all_rmse:
            print(f"  ðŸ“Š RMSE: {np.mean(all_rmse):.4f} Â± {np.std(all_rmse):.4f}")
        
        if all_direction:
            print(f"  ðŸ“Š Direction Accuracy: {np.mean(all_direction):.1f}% Â± {np.std(all_direction):.1f}%")
            good_direction = sum(1 for acc in all_direction if acc > 55)
            print(f"     Above 55%: {good_direction}/{len(all_direction)} ({good_direction/len(all_direction)*100:.1f}%)")

def check_predictions():
    """Check if predictions were generated"""
    print_section("Prediction Status")
    
    # Check for prediction outputs
    outputs_dir = "outputs"
    if os.path.exists(outputs_dir):
        prediction_files = [f for f in os.listdir(outputs_dir) if 'prediction' in f.lower()]
        print(f"ðŸ“ Prediction files: {len(prediction_files)}")
        
        if prediction_files:
            # Check latest predictions
            latest_pred = sorted(prediction_files)[-1]
            pred_path = os.path.join(outputs_dir, latest_pred)
            
            try:
                if latest_pred.endswith('.csv'):
                    df = pd.read_csv(pred_path)
                    print(f"ðŸ“Š Latest predictions: {latest_pred}")
                    print(f"   Shape: {df.shape}")
                    print(f"   Columns: {list(df.columns)}")
                    
                    # Show sample predictions
                    if len(df) > 0:
                        print(f"   Sample predictions:")
                        print(df.head(3).to_string(index=False))
                        
            except Exception as e:
                print(f"âŒ Error reading predictions: {e}")
        else:
            print("âš ï¸ No prediction files found")
    else:
        print("âš ï¸ Outputs directory not found")

def generate_summary_report():
    """Generate a summary report"""
    print_section("Summary Report")
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Count files
    model_count = len([f for f in os.listdir("models") if f.endswith('.pt')]) if os.path.exists("models") else 0
    data_files = len([f for f in os.listdir("data/raw") if f.endswith('.csv')]) if os.path.exists("data/raw") else 0
    
    summary = f"""
# Banking Stock Prediction - Results Summary

**Generated:** {timestamp}

## Pipeline Status
- **Data Files:** {data_files} CSV files collected
- **Trained Models:** {model_count} model files
- **Latest Run:** {datetime.now().strftime('%Y-%m-%d')}

## Key Findings
- Models show directional predictive power (60-87% accuracy)
- Transformer model generally performs best
- Some models achieve RÂ² > 0.6 for certain banks
- Regular retraining recommended for best performance

## Next Steps
1. Monitor model performance over time
2. Consider ensemble methods for better stability
3. Focus on direction prediction for trading signals
4. Update data regularly (daily/weekly)

## Maintenance
- Run `python run_full_pipeline.py` for complete retraining
- Run `python check_results.py` to monitor performance
- Check logs in `logs/` directory for detailed information
"""
    
    # Save summary
    with open("pipeline_summary.md", "w", encoding='utf-8') as f:
        f.write(summary)
    
    print("Summary report saved to: pipeline_summary.md")

def main():
    """Main results checking function"""
    print_header("BANKING STOCK PREDICTION - RESULTS CHECK")
    print(f"Check time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Run all checks
    check_data_status()
    model_stats = check_model_status()
    analyze_training_logs()
    check_predictions()
    generate_summary_report()
    
    # Final assessment
    print_header("FINAL ASSESSMENT")
    
    total_models = sum(model_stats.values()) if model_stats else 0
    
    if total_models > 0:
        print("PIPELINE SUCCESS!")
        print(f"{total_models} models trained successfully")
        print("Check pipeline_summary.md for detailed report")
        
        # Recommendations
        print(f"\nRecommendations:")
        print("- Use Transformer models for best performance")
        print("- Focus on direction accuracy over exact price prediction")
        print("- Run pipeline weekly to keep models updated")
        print("- Monitor logs for any training issues")
        
    else:
        print("PIPELINE INCOMPLETE")
        print("No trained models found")
        print("Run 'python run_full_pipeline.py' to train models")

if __name__ == "__main__":
    main()